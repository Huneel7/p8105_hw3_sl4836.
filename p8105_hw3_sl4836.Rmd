---
title: "p8105_hw3_sl4836"
author: "Hun"
date: "10/16/2021"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(readxl)
library(dplyr)
```

### ggplot theme template from Dr.Goldsmith github
```{r, warning=FALSE, message=FALSE, echo=FALSE}
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Problem 0
```{r, warning=FALSE, message=FALSE}
getwd()

dir.create(file.path(getwd(), "HW3_data_file"), recursive = TRUE)

list.files()
```

### Problem 1_Import Data and Data_Summary
```{r, warning=FALSE, message=FALSE}
library(p8105.datasets)
data("instacart")
```

### Problem 1_Data_Summary
```{r, warning=FALSE, message=FALSE}
instacart <- instacart 

instacart %>% head

instacart_names <- names(instacart)
instacart_nrow <- nrow(instacart)
instacart_ncol <- ncol(instacart)
```
The size of the dataset is **`r instacart_nrow` x**  **`r instacart_ncol`** and **`r instacart_ncol`** variables: *`r instacart_names`.* There are **`r instacart_nrow`** number of observations without missing data. Among these, there are 4 character variables and 11 numeric variables. 


### Problem 1_(a)_Answering how many aisles are there, and which aisles are the most items ordered from.
```{r, warning=FALSE, message=FALSE}
Arranged_aisles <- 
  instacart %>% 
  group_by(aisle) %>% 
  summarise(n = n()) %>% 
  arrange(desc(n))

number_aisles <- 
  nrow(Arranged_aisles)

top_3_item <- 
  Arranged_aisles %>% 
  mutate(aisle = str_to_title(aisle)) %>% 
  pull(aisle) %>%
  head(3)

top_item <- 
  Arranged_aisles %>% 
  mutate(aisle = str_to_title(aisle)) %>% 
  pull(aisle) %>%
  first()
```
There are **`r number_aisles`** aisles. Among them, top 3 aisles that have the most ordered items are **`r top_3_item`.** It follows that **`r top_item`** has the largest number of ordered items. 

### Problem 1_(b)_Making a plot that shows the number of items ordered in each aisle
```{r, warning=FALSE, message=FALSE}
Arranged_aisles %>% 
  filter(n>10000) %>%
  ggplot(aes(x = reorder(aisle, n), y = n)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  labs(y = "Number of Items", 
       x = "Aisles", 
       title = "The Number of Items Ordered in Each Aisle",
       subtitle = "From aisles that have at least more than 10,000 items") +
  theme(axis.text.y = element_text(size =7.5, face="bold"))

```

This plot shows the number of items ordered in each aisles with at least 10,000 items in a descending order. As aforementioned from the previous part, fresh vegetables aisle has the largest number of items, followed by fresh fruits aisle, packaged vegetables fruits aisle, yogurt aisle, etc. 




### Problem 1_(c)_making a table showing the three most popular items in each aisles

```{r, warning=FALSE, message=FALSE}
options(knitr.kable.NA = 0)

baking_top3 <- instacart %>% 
  filter(aisle == "baking ingredients") %>% 
  group_by(product_name) %>% 
  summarise(n = n()) %>% 
  arrange(desc(n)) %>% 
  slice(1:3) %>%
  mutate(aisle = "baking ingredients")

dog_food_top3 <- instacart %>% 
  filter(aisle == "dog food care") %>% 
  group_by(product_name) %>% 
  summarise(n = n()) %>% 
  arrange(desc(n)) %>% 
  slice(1:3) %>%
  mutate(aisle = "dog food care")

packaged_vege_fruit_top3 <- instacart %>% 
  filter(aisle == "packaged vegetables fruits") %>% 
  group_by(product_name) %>% 
  summarise(n = n()) %>% 
  arrange(desc(n)) %>% 
  slice(1:3) %>%
  mutate(aisle = "packaged vegetables fruits")

bind_rows(baking_top3 ,
          dog_food_top3, 
          packaged_vege_fruit_top3)  %>%
    mutate(aisle = str_to_title(aisle)) %>%
  rename(Aisle = aisle) %>%
  knitr::kable(align = "c", format = "pipe", 
               caption = "**Table 1: Three Most Popular Items 
               with their counts in Each Aisles**")
```
This table shows three most popular items with their counts in eash Aisles. In Baking Ingredients aisle, **light brown sugar, pure baking soda, and cande sugar** are top 3 popular items. In Dog Food Care aisle, **snack sticks chicken & rice recipe dog treats, organix chicken & brown rice recipe, and small dog biscuits** are top 3 popular items. In Packaged Vegetables Fruits aisle, **organic baby spinach, organic raspberries, and organic blueberries** are top 3 popular items.

### Problem 1_(d)_Making a table showing the mean hour of the day
```{r, warning=FALSE, message=FALSE}
instacart %>% 
  filter(product_name == "Pink Lady Apples" |product_name == "Coffee Ice Cream") %>%
  group_by(product_name, order_dow) %>% 
  summarise(mean_hour = mean(order_hour_of_day)) %>%
  rename(Product_Name = product_name) %>%
  pivot_wider(names_from = order_dow, values_from = mean_hour) %>%
  knitr::kable(align = "c", format = "pipe",
               caption = "**The Mean Hour of the Day at which
               Each Item is Ordered on Each Day**"
               )
  
```
This tables shows the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. For Coffee Ice Cream, Day 2 has the longest mean hour whereas Day 5 has the least mean hour. For Pink Lady Apples, Day 3 has the longest mean hour whereas Day 1 has least mean hour. 

### Problem 2_Data Import
```{r, warning=FALSE, message=FALSE}
data("brfss_smart2010") 
```

### Problem 2_(a)_Data_Cleaning
```{r, warning=FALSE, message=FALSE}
brfss <- brfss_smart2010 


brfss_clean <- brfss %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>%
  filter(response %in% c("Excellent", "Very good",
                         "Good","Fair", "Poor" )) %>%
  mutate(response = as.factor(response) %>% 
           fct_relevel("Poor", "Fair", "Good", "Very good",
                       "Excellent")) 

```



After filtering Overall Health level solely from Topic variable, I included responses from Excellent to Poor. And then I re-leveled the response variable from Poor to Excellent.

### Problem 2_(b)_Showing States observed at 7 or more observations in 2002
```{r, warning=FALSE, message=FALSE}

brfss_clean_2002 <-
  brfss_clean %>% 
  rename(States = locationabbr) %>%
  filter(year == 2002) %>% 
  group_by(States) %>% 
  summarise(n = n_distinct(locationdesc)) %>% 
  filter(n >= 7) %>%
  arrange(n)

brfss_clean_2002_States <- brfss_clean_2002 %>% pull(States)  
brfss_clean_2002 %>% knitr::kable()



```

 **`r brfss_clean_2002_States`** are the states that were observed at 7 or more observations in 2002. 

### Problem 2_(b)_Showing States observed at 7 or more observations in 2010
```{r, warning=FALSE, message=FALSE}
brfss_clean_2010 <-
  brfss_clean %>% 
  rename(States = locationabbr) %>%
  filter(year == 2010) %>%
  group_by(States) %>% 
  summarise(n = n_distinct(locationdesc)) %>% 
  filter(n >= 7) %>%
  arrange(n) 
  
  
brfss_clean_2010_States <- brfss_clean_2010 %>% pull(States)  
brfss_clean_2010 %>% knitr::kable()
  

```
 **`r brfss_clean_2010_States`** are the states that were observed at 7 or more observations in 2010. 


### Problem 2_(c)_Making a spaghetti plot of BRFSS data average value over time within a state 
```{r, warning=FALSE, message=FALSE}
brfss_clean %>% 
  filter(response == "Excellent") %>%
  rename(States = locationabbr) %>%
  group_by(year, States) %>%
  summarise(mean_data_value = mean(data_value, na.rm=T)) %>% 
  ggplot(aes(year, mean_data_value, color=States)) + 
  geom_point(size=0.2) + 
  geom_line(aes(group=States), alpha=0.5) +
  labs(y = "Average Value", x = "Year") +
  theme(legend.position = "right", legend.title =
          element_text(colour="black", size=10, face="bold"), 
        legend.key.size = unit(0.5, 'cm')) 
 

```

This is a spaghetti plot that shows a line of the average BRFSS data value over time within a state from 2002 to 2010. It appears that the average values are relatively high in 2002 across states. 


### Problem 2_(d)_Making the distribution of BRFSS data_value for responses in 2006 and 2010
```{r, warning=FALSE, message=FALSE}
library(ggridges)

brfss_clean %>% 
  filter(year == 2006 | year == 2010, locationabbr == "NY") %>%
  group_by(locationdesc) %>%
  ggplot(aes(response, data_value, color = response)) +
  geom_boxplot() +
  facet_wrap(.~year) + 
  ggtitle("Distribution of data_value for responses in NY State") +
  labs(x = "Levels of Response", y = "BRFSS Data Value") +
  scale_color_discrete(name="Levels of Response:") +
  theme(axis.text.x = element_text(size=8))
```

This twp-panel plot shows the distribution of BRFSS data value for responses from Poor to Excellent in 2006 and 2010. It appears that there is a similar overall trend between 2006 and 2010, but the distribution of data value is quite different within response levels between 2006 and 2010. 

### Problem 3_(a)_Loading, Tidying, and Wrangling Data
```{r, warning=FALSE, message=FALSE}
accel <- read.csv("./hw3_data_file/accel_data.csv") 

clean_accel <- accel %>% 
  janitor::clean_names() %>% 
  pivot_longer(activity_1:activity_1440, 
               names_to = "minutes_in_a_day", values_to = "activity_count", names_prefix =  "activity_") %>%
  mutate(day = day %>% 
           fct_relevel("Sunday", "Monday", "Tuesday", "Wednesday","Thursday","Friday", "Saturday")) %>% 
  mutate(weekday_vs_weekend = ifelse(day %in% c("Monday", "Tuesday", "Wednesday", "Thursday","Friday"), "weekday","weekend")) %>% 
  mutate_if(is.double, as.integer) %>%
  mutate(minutes_in_a_day = as.integer(minutes_in_a_day))
  
```

### Problem 3_(a)_Describing the resulting dataset using in-line
```{r, warning=FALSE, message=FALSE}
accel_row <- accel %>% nrow()
accel_col <- accel %>% ncol()
accel_names <- accel %>% names()


clean_accel_row <- clean_accel %>% nrow()
clean_accel_col <- clean_accel %>% ncol()
clean_accel_names <- clean_accel %>% names()

```

Originally, the dimension of the pols_month_data was **`r accel_row` x**  **`r accel_col`** and there are **`r accel_col`** variables. This represents one person's activity data for **`r accel_row`** days. After tidying and wrangling the data, the dimension is **`r clean_accel_row` x**  **`r clean_accel_col`** and **`r clean_accel_col`** variables: *`r clean_accel_names`.* There are **`r clean_accel_row`** number of observations, this total number is indeed the number of combined data of each 1440 minutes of 35 days, namely **35x1440 =** **`r clean_accel_row`** from original data.  


### Problem 3_(b)_Creating a table that shows aggregation of a total activity for each day
```{r, warning=FALSE, message=FALSE}
clean_accel %>% 
  group_by(day) %>% 
  summarize(total = sum(activity_count)) %>% 
  rename(Day = day, Total = total) %>%
  knitr::kable(align = "c", format = "pipe", caption = "**Table 1:
               Aggregation of a Total Activity across Minutes for Each Day**")

```

According to the Table 1, it's hard to say there are apparent trends across days except that the total activity aggregation on Saturday is far lower than any other days as one can expect people normally would like to rest on Saturday. It is also to be observed the total activity on Friday has the highest aggregation.




### Part 3_(c)_Making a single-panel plot that shows the 24-hour activity time courses for each day.
```{r, warning=FALSE, message=FALSE}
clean_accel %>%
  group_by(day, minutes_in_a_day) %>%
  rename(Day = day) %>%
  ggplot(aes(minutes_in_a_day, activity_count, color = Day)) +
  geom_point() +
  scale_x_continuous(
    breaks = c(0, 180, 360, 540, 720, 900, 1080, 1260, 1440),
    labels = c("0hr", "3hr", "6hr", "9hr", "12hr", "15hr", "18hr", "21hr", "24hr")) + 
  labs(x = "Hours", y="Activity Count") +
  ggtitle("24-hour Activity Time Courses for Each Day with one-minute interval") + 
  theme(plot.title = element_text(size=12))

```

Between 0hr (12am) to 6hr (6am), it is to be observed that the activity count of a 63 year-old male is the lowest across days as one can expect people normally sleep during those times. Around 7hr (7am), his activity count is relatively high on some Thursdays. Around 9hr (9am), his activity count is quite high on some Fridays. Around 12hr (12pm), his activity count is high on many Sundays. Between 16hr (4pm) and 17hr (5pm), his activity count is relatively high on a decent number of weekends. Between 20hr (8pm) and 22hr(10pm), his activity count is high across many days, especially on Friday followed by Saturday, Wednesday, and Monday. 

